# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttZeWdLonUoazZ1HkBoPsiA5FBYzqqeU

# 1. Data Loading and Exploration
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Advanced ML and Analysis Libraries
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.svm import SVR, SVC
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier

# Time Series and Forecasting
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
import scipy.stats as stats
from scipy.spatial.distance import pdist, squareform

# Visualization
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

"""#EXTENSIVE EXPLORATORY DATA ANALYSIS"""

df = pd.read_csv('data.csv')
print(f"Dataset loaded: {df.shape[0]} records, {df.shape[1]} features")

# Enhanced data preprocessing
df['DateOfCall'] = pd.to_datetime(df['DateOfCall'], format='%d-%b-%y', errors='coerce')
df['TimeOfCall'] = pd.to_datetime(df['TimeOfCall'], format='%H:%M:%S', errors='coerce').dt.time

# Create comprehensive temporal features
df['Year'] = df['DateOfCall'].dt.year
df['Month'] = df['DateOfCall'].dt.month
df['DayOfWeek'] = df['DateOfCall'].dt.dayofweek
df['DayOfYear'] = df['DateOfCall'].dt.dayofyear
df['Quarter'] = df['DateOfCall'].dt.quarter
df['WeekOfYear'] = df['DateOfCall'].dt.isocalendar().week
df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)
df['IsNight'] = ((df['HourOfCall'] >= 22) | (df['HourOfCall'] <= 6)).astype(int)
df['IsRushHour'] = ((df['HourOfCall'].between(7, 9)) | (df['HourOfCall'].between(17, 19))).astype(int)
df['IsPeakSeason'] = df['Month'].isin([6, 7, 8, 12]).astype(int)  # Summer and December

# Enhanced geographic features
df['Distance_from_center'] = np.sqrt(
    (df['Easting_m'] - df['Easting_m'].mean())**2 +
    (df['Northing_m'] - df['Northing_m'].mean())**2
)

# Handle missing values in critical fields
df['SecondPumpArriving_AttendanceTime'] = df['SecondPumpArriving_AttendanceTime'].replace('NULL', np.nan)
df['SecondPumpArriving_AttendanceTime'] = pd.to_numeric(df['SecondPumpArriving_AttendanceTime'], errors='coerce')
df['HasSecondPump'] = (~df['SecondPumpArriving_AttendanceTime'].isna()).astype(int)

# Fill missing values strategically
df['SpecialServiceType'] = df['SpecialServiceType'].fillna('Standard')
df['Postcode_full'] = df['Postcode_full'].fillna('Unknown')
df['FirstPumpArriving_DeployedFromStation'] = df['FirstPumpArriving_DeployedFromStation'].fillna('Unknown')
df['SecondPumpArriving_DeployedFromStation'] = df['SecondPumpArriving_DeployedFromStation'].fillna('None')

print("Enhanced preprocessing completed.")
print(f"Temporal features created: Year, Month, DayOfWeek, Quarter, WeekOfYear, etc.")
print(f"Geographic features: Distance calculations, coordinate analysis")
print(f"Missing values handled strategically")

"""#2. CRITICAL TRENDS AND PATTERNS ANALYSIS"""

# 2.1 TEMPORAL TRENDS ANALYSIS
print("\n2.1 TEMPORAL TRENDS ANALYSIS")

# Create comprehensive temporal analysis
temporal_analysis = df.groupby(['Year', 'Month', 'DayOfWeek', 'HourOfCall']).agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': ['mean', 'std'],
    'Notional Cost (£)': ['mean', 'sum'],
    'NumPumpsAttending': 'mean',
    'HasSecondPump': 'mean'
}).round(2)

# Incident frequency patterns
print("INCIDENT FREQUENCY PATTERNS:")
hourly_incidents = df.groupby('HourOfCall')['IncidentNumber'].count()
daily_incidents = df.groupby('DayOfWeek')['IncidentNumber'].count()
monthly_incidents = df.groupby('Month')['IncidentNumber'].count()

print(f"Peak incident hour: {hourly_incidents.idxmax()}:00 ({hourly_incidents.max()} incidents)")
print(f"Peak incident day: Day {daily_incidents.idxmax()} ({daily_incidents.max()} incidents)")
print(f"Peak incident month: Month {monthly_incidents.idxmax()} ({monthly_incidents.max()} incidents)")

# Response time patterns
response_patterns = df.groupby(['HourOfCall', 'DayOfWeek'])['FirstPumpArriving_AttendanceTime'].agg(['mean', 'count'])
print(f"\nAverage response time: {df['FirstPumpArriving_AttendanceTime'].mean():.1f} seconds")
print(f"Response time std dev: {df['FirstPumpArriving_AttendanceTime'].std():.1f} seconds")

# 2.2 GEOGRAPHIC PATTERNS ANALYSIS
print("\n2.2 GEOGRAPHIC PATTERNS ANALYSIS")

geographic_analysis = df.groupby(['IncGeo_BoroughName', 'IncGeo_WardName']).agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean',
    'Notional Cost (£)': 'mean',
    'Distance_from_center': 'mean'
}).round(2)

print("GEOGRAPHIC HOTSPOTS:")
borough_incidents = df['IncGeo_BoroughName'].value_counts()
print(f"Top 3 boroughs by incidents:")
for i, (borough, count) in enumerate(borough_incidents.head(3).items(), 1):
    print(f"  {i}. {borough}: {count} incidents")

# 2.3 INCIDENT TYPE ANALYSIS
print("\n2.3 INCIDENT TYPE ANALYSIS")

incident_analysis = df.groupby('IncidentGroup').agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': ['mean', 'std'],
    'Notional Cost (£)': ['mean', 'sum'],
    'NumPumpsAttending': 'mean',
    'HasSecondPump': 'mean'
}).round(2)

print("INCIDENT TYPE BREAKDOWN:")
for incident_type in df['IncidentGroup'].unique():
    count = df[df['IncidentGroup'] == incident_type].shape[0]
    avg_response = df[df['IncidentGroup'] == incident_type]['FirstPumpArriving_AttendanceTime'].mean()
    print(f"  {incident_type}: {count} incidents, avg response: {avg_response:.1f}s")

# 2.4 CORRELATION ANALYSIS
print("\n2.4 CORRELATION ANALYSIS")

# Select numeric columns for correlation
numeric_cols = ['HourOfCall', 'Month', 'DayOfWeek', 'FirstPumpArriving_AttendanceTime',
                'NumStationsWithPumpsAttending', 'NumPumpsAttending', 'PumpCount',
                'PumpMinutesRounded', 'Notional Cost (£)', 'NumCalls', 'Distance_from_center',
                'IsWeekend', 'IsNight', 'IsRushHour', 'HasSecondPump']

correlation_matrix = df[numeric_cols].corr()

# Find strongest correlations
correlation_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_val = correlation_matrix.iloc[i, j]
        if abs(corr_val) > 0.3:  # Only show strong correlations
            correlation_pairs.append((
                correlation_matrix.columns[i],
                correlation_matrix.columns[j],
                corr_val
            ))

correlation_pairs.sort(key=lambda x: abs(x[2]), reverse=True)

print("STRONGEST CORRELATIONS (|r| > 0.3):")
for var1, var2, corr in correlation_pairs[:10]:
    print(f"  {var1} ↔ {var2}: {corr:.3f}")

"""#3. ADVANCED VISUALIZATION DASHBOARD"""

# Create comprehensive visualization dashboard
fig = plt.figure(figsize=(25, 20))
fig.suptitle('COMPREHENSIVE FIRE SERVICE ANALYSIS DASHBOARD', fontsize=24, fontweight='bold', y=0.98)

# 3.1 Temporal Pattern Analysis
ax1 = plt.subplot(3, 4, 1)
hourly_response = df.groupby('HourOfCall').agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean'
})
ax1_twin = ax1.twinx()
line1 = ax1.plot(hourly_response.index, hourly_response['IncidentNumber'], 'b-o', linewidth=2, label='Incident Count')
line2 = ax1_twin.plot(hourly_response.index, hourly_response['FirstPumpArriving_AttendanceTime'], 'r-s', linewidth=2, label='Avg Response Time')
ax1.set_xlabel('Hour of Day')
ax1.set_ylabel('Incident Count', color='blue')
ax1_twin.set_ylabel('Response Time (sec)', color='red')
ax1.set_title('Hourly Patterns: Incidents vs Response Time')
ax1.grid(True, alpha=0.3)

# 3.2 Geographic Hotspot Analysis
ax2 = plt.subplot(3, 4, 2)
borough_stats = df.groupby('IncGeo_BoroughName').agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean'
}).sort_values('IncidentNumber', ascending=False).head(10)
bars = ax2.bar(range(len(borough_stats)), borough_stats['IncidentNumber'],
               color=plt.cm.viridis(borough_stats['FirstPumpArriving_AttendanceTime'] / borough_stats['FirstPumpArriving_AttendanceTime'].max()))
ax2.set_xticks(range(len(borough_stats)))
ax2.set_xticklabels(borough_stats.index, rotation=45, ha='right', fontsize=8)
ax2.set_title('Geographic Hotspots (Top 10 Boroughs)')
ax2.set_ylabel('Incident Count')

# 3.3 Incident Type Distribution with Response Times
ax3 = plt.subplot(3, 4, 3)
incident_stats = df.groupby('IncidentGroup').agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean'
})
bars = ax3.bar(incident_stats.index, incident_stats['IncidentNumber'],
               color=['red', 'blue', 'green'][:len(incident_stats)])
ax3.set_title('Incident Type Distribution')
ax3.set_ylabel('Count')
ax3.tick_params(axis='x', rotation=45)
# Add response time labels on bars
for i, (idx, row) in enumerate(incident_stats.iterrows()):
    ax3.text(i, row['IncidentNumber'] + 0.5, f"{row['FirstPumpArriving_AttendanceTime']:.0f}s",
             ha='center', va='bottom', fontsize=10, fontweight='bold')

# 3.4 Cost Analysis
ax4 = plt.subplot(3, 4, 4)
cost_analysis = df.groupby('PropertyCategory')['Notional Cost (£)'].agg(['mean', 'sum']).sort_values('sum', ascending=False)
ax4.bar(range(len(cost_analysis)), cost_analysis['sum'], color='orange', alpha=0.7)
ax4.set_xticks(range(len(cost_analysis)))
ax4.set_xticklabels(cost_analysis.index, rotation=45, ha='right')
ax4.set_title('Total Cost by Property Category')
ax4.set_ylabel('Total Cost (£)')

# 3.5 Resource Deployment Analysis
ax5 = plt.subplot(3, 4, 5)
resource_analysis = df.groupby('NumPumpsAttending').agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean',
    'Notional Cost (£)': 'mean'
})
ax5.scatter(resource_analysis['FirstPumpArriving_AttendanceTime'], resource_analysis['Notional Cost (£)'],
           s=resource_analysis['IncidentNumber']*50, alpha=0.6, c=resource_analysis.index, cmap='viridis')
ax5.set_xlabel('Avg Response Time (sec)')
ax5.set_ylabel('Avg Cost (£)')
ax5.set_title('Resource Deployment Efficiency\n(Size=Incident Count, Color=Pump Count)')

# 3.6 Seasonal Patterns
ax6 = plt.subplot(3, 4, 6)
seasonal_data = df.groupby('Month').agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean'
})
ax6.plot(seasonal_data.index, seasonal_data['IncidentNumber'], 'o-', linewidth=2, markersize=8)
ax6.set_title('Seasonal Incident Patterns')
ax6.set_xlabel('Month')
ax6.set_ylabel('Incident Count')
ax6.grid(True, alpha=0.3)

# 3.7 Response Time Distribution by Day Type
ax7 = plt.subplot(3, 4, 7)
weekend_response = df[df['IsWeekend'] == 1]['FirstPumpArriving_AttendanceTime']
weekday_response = df[df['IsWeekend'] == 0]['FirstPumpArriving_AttendanceTime']
ax7.hist([weekday_response, weekend_response], bins=15, alpha=0.7, label=['Weekday', 'Weekend'])
ax7.set_title('Response Time: Weekday vs Weekend')
ax7.set_xlabel('Response Time (seconds)')
ax7.set_ylabel('Frequency')
ax7.legend()

# 3.8 Property Type Risk Assessment
ax8 = plt.subplot(3, 4, 8)
property_risk = df.groupby('PropertyType').agg({
    'IncidentNumber': 'count',
    'HasSecondPump': 'mean',
    'Notional Cost (£)': 'mean'
}).sort_values('IncidentNumber', ascending=False).head(8)
ax8.scatter(property_risk['HasSecondPump'], property_risk['Notional Cost (£)'],
           s=property_risk['IncidentNumber']*20, alpha=0.6)
ax8.set_xlabel('Second Pump Rate')
ax8.set_ylabel('Avg Cost (£)')
ax8.set_title('Property Risk Assessment\n(Size=Incident Frequency)')

# 3.9 Station Performance Analysis
ax9 = plt.subplot(3, 4, 9)
station_performance = df.groupby('IncidentStationGround').agg({
    'FirstPumpArriving_AttendanceTime': 'mean',
    'IncidentNumber': 'count'
}).sort_values('FirstPumpArriving_AttendanceTime').head(10)
bars = ax9.barh(range(len(station_performance)), station_performance['FirstPumpArriving_AttendanceTime'],
                color=plt.cm.RdYlGn_r(station_performance['FirstPumpArriving_AttendanceTime'] / station_performance['FirstPumpArriving_AttendanceTime'].max()))
ax9.set_yticks(range(len(station_performance)))
ax9.set_yticklabels(station_performance.index, fontsize=8)
ax9.set_title('Best Performing Stations\n(by Response Time)')
ax9.set_xlabel('Avg Response Time (sec)')

# 3.10 Time-Distance Relationship
ax10 = plt.subplot(3, 4, 10)
ax10.scatter(df['Distance_from_center'], df['FirstPumpArriving_AttendanceTime'],
            c=df['NumPumpsAttending'], alpha=0.6, cmap='plasma')
ax10.set_xlabel('Distance from Center')
ax10.set_ylabel('Response Time (sec)')
ax10.set_title('Distance vs Response Time\n(Color=Pump Count)')

# 3.11 Incident Severity Analysis
ax11 = plt.subplot(3, 4, 11)
severity_proxy = df.groupby(pd.cut(df['PumpMinutesRounded'], bins=5)).agg({
    'IncidentNumber': 'count',
    'Notional Cost (£)': 'mean'
})
ax11.bar(range(len(severity_proxy)), severity_proxy['IncidentNumber'],
         color=plt.cm.Reds(severity_proxy['Notional Cost (£)'] / severity_proxy['Notional Cost (£)'].max()))
ax11.set_title('Incident Severity Distribution\n(by Pump Minutes)')
ax11.set_xlabel('Severity Categories')
ax11.set_ylabel('Incident Count')

# 3.12 Multi-dimensional Performance
ax12 = plt.subplot(3, 4, 12)
performance_data = df.groupby('HourOfCall').agg({
    'FirstPumpArriving_AttendanceTime': 'mean',
    'Notional Cost (£)': 'mean',
    'NumPumpsAttending': 'mean'
}).reset_index()
ax12.plot(performance_data['HourOfCall'], performance_data['FirstPumpArriving_AttendanceTime'], 'r-o', label='Response Time', linewidth=2)
ax12_twin = ax12.twinx()
ax12_twin.plot(performance_data['HourOfCall'], performance_data['Notional Cost (£)'], 'b-s', label='Cost', linewidth=2)
ax12.set_xlabel('Hour of Day')
ax12.set_ylabel('Response Time (sec)', color='red')
ax12_twin.set_ylabel('Cost (£)', color='blue')
ax12.set_title('Multi-dimensional Performance')
ax12.grid(True, alpha=0.3)

plt.tight_layout()
plt.subplots_adjust(top=0.95)
plt.show()

"""# 4.MACHINE LEARNING MODELS FOR FORECASTING"""

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor
from sklearn.svm import SVR, SVC
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.cluster import KMeans, DBSCAN

# 4.1 PREPARE FEATURES FOR COMPREHENSIVE MODELING
print("\n4.1 PREPARING FEATURES FOR ADVANCED MODELING")

# Encode categorical variables
label_encoders = {}
categorical_cols = ['IncidentGroup', 'PropertyCategory', 'PropertyType', 'IncGeo_BoroughName',
                   'IncGeo_WardName', 'IncidentStationGround', 'FirstPumpArriving_DeployedFromStation',
                   'SpecialServiceType', 'Postcode_district']

df_model = df.copy()
for col in categorical_cols:
    if col in df_model.columns:
        le = LabelEncoder()
        df_model[col + '_encoded'] = le.fit_transform(df_model[col].astype(str))
        label_encoders[col] = le

# Create comprehensive feature set
feature_cols = [
    # Temporal features
    'HourOfCall', 'Month', 'DayOfWeek', 'Quarter', 'WeekOfYear', 'DayOfYear',
    'IsWeekend', 'IsNight', 'IsRushHour', 'IsPeakSeason',

    # Categorical encoded features
    'IncidentGroup_encoded', 'PropertyCategory_encoded', 'PropertyType_encoded',
    'IncGeo_BoroughName_encoded', 'IncGeo_WardName_encoded',
    'IncidentStationGround_encoded', 'FirstPumpArriving_DeployedFromStation_encoded',

    # Operational features
    'NumStationsWithPumpsAttending', 'NumPumpsAttending', 'PumpCount',
    'Distance_from_center',

    # Geographic features
    'Easting_m', 'Northing_m', 'Latitude', 'Longitude'
]

# Filter existing features
existing_features = [col for col in feature_cols if col in df_model.columns]
print(f"Using {len(existing_features)} features for modeling")

# Clean data for modeling
df_clean = df_model.dropna(subset=['FirstPumpArriving_AttendanceTime', 'Notional Cost (£)'])
print(f"Dataset size after cleaning: {len(df_clean)} records")

X = df_clean[existing_features]
y_response_time = df_clean['FirstPumpArriving_AttendanceTime']
y_cost = df_clean['Notional Cost (£)']
y_incident_type = df_clean['IncidentGroup_encoded']

# Handle any remaining missing values
if X.isna().any().any():
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())

# 4.2 INCIDENT FREQUENCY FORECASTING MODEL
print("\n4.2 INCIDENT FREQUENCY FORECASTING MODEL")

# Create aggregated data for frequency forecasting
frequency_data = df_clean.groupby(['Year', 'Month', 'DayOfWeek', 'HourOfCall']).agg({
    'IncidentNumber': 'count',
    'IncidentGroup_encoded': lambda x: x.mode()[0] if not x.empty else 0,
    'IncGeo_BoroughName_encoded': lambda x: x.mode()[0] if not x.empty else 0
}).reset_index()

frequency_features = ['Year', 'Month', 'DayOfWeek', 'HourOfCall', 'IncidentGroup_encoded', 'IncGeo_BoroughName_encoded']
X_freq = frequency_data[frequency_features]
y_freq = frequency_data['IncidentNumber']

# Split data
X_freq_train, X_freq_test, y_freq_train, y_freq_test = train_test_split(
    X_freq, y_freq, test_size=0.2, random_state=42
)

# Train frequency prediction models
frequency_models = {}

# Random Forest for Frequency
rf_freq = RandomForestRegressor(n_estimators=100, random_state=42)
rf_freq.fit(X_freq_train, y_freq_train)
frequency_models['Random Forest'] = rf_freq

# Gradient Boosting for Frequency
gb_freq = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_freq.fit(X_freq_train, y_freq_train)
frequency_models['Gradient Boosting'] = gb_freq

print("✓ Incident frequency forecasting models trained")

# 4.3 INCIDENT TYPE PREDICTION MODEL
print("\n4.3 INCIDENT TYPE PREDICTION MODEL")

# Split data for type prediction
X_train, X_test, y_type_train, y_type_test = train_test_split(
    X, y_incident_type, test_size=0.2, random_state=42
)

# Scale features
scaler_type = StandardScaler()
X_train_scaled = scaler_type.fit_transform(X_train)
X_test_scaled = scaler_type.transform(X_test)

# Train type prediction models
type_models = {}

# Random Forest Classifier
rf_type = RandomForestClassifier(n_estimators=100, random_state=42)
rf_type.fit(X_train, y_type_train)
type_models['Random Forest'] = rf_type

# Support Vector Machine
svm_type = SVC(random_state=42, probability=True)
svm_type.fit(X_train_scaled, y_type_train)
type_models['SVM'] = svm_type

# K-Nearest Neighbors
knn_type = KNeighborsClassifier(n_neighbors=5)
knn_type.fit(X_train_scaled, y_type_train)
type_models['KNN'] = knn_type

print("✓ Incident type prediction models trained")

# 4.4 LOCATION PREDICTION MODEL (CLUSTERING)
print("\n4.4 LOCATION PREDICTION MODEL (CLUSTERING)")

# Geographic clustering for location prediction
location_features = ['Latitude', 'Longitude', 'Distance_from_center', 'IncGeo_BoroughName_encoded']
X_location = df_clean[location_features].dropna()

# K-means clustering for location zones
kmeans_location = KMeans(n_clusters=5, random_state=42)
location_clusters = kmeans_location.fit_predict(X_location)

# DBSCAN for density-based clustering
dbscan_location = DBSCAN(eps=0.01, min_samples=2)
dbscan_clusters = dbscan_location.fit_predict(X_location)

print(f"✓ Location clustering completed: {len(np.unique(location_clusters))} K-means clusters, {len(np.unique(dbscan_clusters))} DBSCAN clusters")

# 4.5 RESPONSE TIME PREDICTION MODEL
print("\n4.5 RESPONSE TIME PREDICTION MODEL")

# Split data for response time prediction
X_rt_train, X_rt_test, y_rt_train, y_rt_test = train_test_split(
    X, y_response_time, test_size=0.2, random_state=42
)

# Scale features
scaler_rt = StandardScaler()
X_rt_train_scaled = scaler_rt.fit_transform(X_rt_train)
X_rt_test_scaled = scaler_rt.transform(X_rt_test)

# Train response time models
rt_models = {}

# Random Forest
rf_rt = RandomForestRegressor(n_estimators=100, random_state=42)
rf_rt.fit(X_rt_train, y_rt_train)
rt_models['Random Forest'] = rf_rt

# Gradient Boosting
gb_rt = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_rt.fit(X_rt_train, y_rt_train)
rt_models['Gradient Boosting'] = gb_rt

# Support Vector Regression
svr_rt = SVR(kernel='rbf')
svr_rt.fit(X_rt_train_scaled, y_rt_train)
rt_models['SVR'] = svr_rt

print("✓ Response time prediction models trained")

"""#5. MODEL EVALUATION AND PERFORMANCE ANALYSIS"""

# 5.1 EVALUATE FREQUENCY FORECASTING MODELS
print("\n5.1 FREQUENCY FORECASTING MODEL PERFORMANCE")

freq_results = {}
for name, model in frequency_models.items():
    pred = model.predict(X_freq_test)
    mse = mean_squared_error(y_freq_test, pred)
    r2 = r2_score(y_freq_test, pred)
    mae = mean_absolute_error(y_freq_test, pred)

    freq_results[name] = {'MSE': mse, 'R2': r2, 'MAE': mae}
    print(f"{name}: MSE={mse:.4f}, R²={r2:.4f}, MAE={mae:.4f}")

# 5.2 EVALUATE TYPE PREDICTION MODELS
print("\n5.2 INCIDENT TYPE PREDICTION MODEL PERFORMANCE")

type_results = {}
for name, model in type_models.items():
    if name == 'Random Forest':
        pred = model.predict(X_test)
    else:
        pred = model.predict(X_test_scaled)

    accuracy = (pred == y_type_test).mean()
    type_results[name] = {'Accuracy': accuracy}
    print(f"{name}: Accuracy={accuracy:.4f}")

# 5.3 EVALUATE RESPONSE TIME MODELS
print("\n5.3 RESPONSE TIME PREDICTION MODEL PERFORMANCE")

rt_results = {}
for name, model in rt_models.items():
    if name in ['Random Forest', 'Gradient Boosting']:
        pred = model.predict(X_rt_test)
    else:
        pred = model.predict(X_rt_test_scaled)

    mse = mean_squared_error(y_rt_test, pred)
    r2 = r2_score(y_rt_test, pred)
    mae = mean_absolute_error(y_rt_test, pred)

    rt_results[name] = {'MSE': mse, 'R2': r2, 'MAE': mae}
    print(f"{name}: MSE={mse:.4f}, R²={r2:.4f}, MAE={mae:.4f}")

# 5.4 FEATURE IMPORTANCE ANALYSIS
print("\n5.4 FEATURE IMPORTANCE ANALYSIS")

# Get feature importance from Random Forest models
if 'Random Forest' in rt_models:
    feature_importance = pd.DataFrame({
        'feature': existing_features,
        'importance': rt_models['Random Forest'].feature_importances_
    }).sort_values('importance', ascending=False)

    print("TOP 10 MOST IMPORTANT FEATURES FOR RESPONSE TIME PREDICTION:")
    for i, row in feature_importance.head(10).iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")

"""# 6. ADVANCED FORECASTING VISUALIZATIONS"""

# Create forecasting visualization dashboard
fig, axes = plt.subplots(3, 3, figsize=(25, 20))
fig.suptitle('MACHINE LEARNING FORECASTING RESULTS DASHBOARD', fontsize=20, fontweight='bold')

# 6.1 Frequency Forecasting Performance
ax1 = axes[0, 0]
freq_pred = frequency_models['Random Forest'].predict(X_freq_test)
ax1.scatter(y_freq_test, freq_pred, alpha=0.6, color='blue')
ax1.plot([y_freq_test.min(), y_freq_test.max()], [y_freq_test.min(), y_freq_test.max()], 'r--', lw=2)
ax1.set_xlabel('Actual Frequency')
ax1.set_ylabel('Predicted Frequency')
ax1.set_title('Incident Frequency Forecasting\n(Actual vs Predicted)')
ax1.grid(True, alpha=0.3)

# 6.2 Response Time Prediction Performance
ax2 = axes[0, 1]
rt_pred = rt_models['Random Forest'].predict(X_rt_test)
ax2.scatter(y_rt_test, rt_pred, alpha=0.6, color='green')
ax2.plot([y_rt_test.min(), y_rt_test.max()], [y_rt_test.min(), y_rt_test.max()], 'r--', lw=2)
ax2.set_xlabel('Actual Response Time (sec)')
ax2.set_ylabel('Predicted Response Time (sec)')
ax2.set_title('Response Time Prediction\n(Actual vs Predicted)')
ax2.grid(True, alpha=0.3)

# 6.3 Feature Importance Visualization
ax3 = axes[0, 2]
if 'feature_importance' in locals():
    top_features = feature_importance.head(10)
    bars = ax3.barh(range(len(top_features)), top_features['importance'], color='orange')
    ax3.set_yticks(range(len(top_features)))
    ax3.set_yticklabels(top_features['feature'], fontsize=8)
    ax3.set_xlabel('Importance Score')
    ax3.set_title('Top 10 Feature Importance\n(Random Forest)')

# 6.4 Model Performance Comparison - Frequency
ax4 = axes[1, 0]
freq_model_names = list(freq_results.keys())
freq_r2_scores = [freq_results[name]['R2'] for name in freq_model_names]
bars = ax4.bar(freq_model_names, freq_r2_scores, color=['skyblue', 'lightgreen'])
ax4.set_ylabel('R² Score')
ax4.set_title('Frequency Forecasting\nModel Comparison')
ax4.set_ylim(0, 1)
for i, v in enumerate(freq_r2_scores):
    ax4.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

# 6.5 Model Performance Comparison - Response Time
ax5 = axes[1, 1]
rt_model_names = list(rt_results.keys())
rt_r2_scores = [rt_results[name]['R2'] for name in rt_model_names]
bars = ax5.bar(rt_model_names, rt_r2_scores, color=['red', 'blue', 'green'])
ax5.set_ylabel('R² Score')
ax5.set_title('Response Time Prediction\nModel Comparison')
ax5.tick_params(axis='x', rotation=45)
for i, v in enumerate(rt_r2_scores):
    ax5.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

# 6.6 Type Prediction Accuracy
ax6 = axes[1, 2]
type_model_names = list(type_results.keys())
type_accuracies = [type_results[name]['Accuracy'] for name in type_model_names]
bars = ax6.bar(type_model_names, type_accuracies, color=['purple', 'orange', 'brown'])
ax6.set_ylabel('Accuracy')
ax6.set_title('Incident Type Prediction\nModel Comparison')
ax6.set_ylim(0, 1)
for i, v in enumerate(type_accuracies):
    ax6.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

# 6.7 Geographic Clustering Results
ax7 = axes[2, 0]
scatter = ax7.scatter(X_location['Longitude'], X_location['Latitude'],
                     c=location_clusters, cmap='viridis', alpha=0.6)
ax7.set_xlabel('Longitude')
ax7.set_ylabel('Latitude')
ax7.set_title('Geographic Clustering\n(K-means, 5 clusters)')
plt.colorbar(scatter, ax=ax7)

# 6.8 Prediction Error Analysis
ax8 = axes[2, 1]
rt_errors = rt_pred - y_rt_test
ax8.hist(rt_errors, bins=15, alpha=0.7, color='red', edgecolor='black')
ax8.axvline(rt_errors.mean(), color='blue', linestyle='--', linewidth=2,
           label=f'Mean Error: {rt_errors.mean():.1f}s')
ax8.set_xlabel('Prediction Error (sec)')
ax8.set_ylabel('Frequency')
ax8.set_title('Response Time Prediction\nError Distribution')
ax8.legend()
ax8.grid(True, alpha=0.3)

# 6.9 Time Series Forecast Simulation
ax9 = axes[2, 2]
# Create simulated time series forecast
hours = range(24)
predicted_incidents = []
for hour in hours:
    # Use the frequency model to predict incidents for each hour
    sample_input = np.array([[2018, 1, 0, hour, 0, 0]])  # Sample parameters
    if sample_input.shape[1] == X_freq_train.shape[1]:
        pred = frequency_models['Random Forest'].predict(sample_input)[0]
        predicted_incidents.append(max(0, pred))
    else:
        # Fallback to historical average if dimensions don't match
        predicted_incidents.append(df_clean.groupby('HourOfCall')['IncidentNumber'].count().get(hour, 0))

ax9.plot(hours, predicted_incidents, 'o-', linewidth=2, markersize=6, color='purple')
ax9.set_xlabel('Hour of Day')
ax9.set_ylabel('Predicted Incidents')
ax9.set_title('24-Hour Incident Forecast\n(Sample Day)')
ax9.grid(True, alpha=0.3)
ax9.set_xticks(range(0, 24, 2))

plt.tight_layout()
plt.show()

"""# 7. STRATEGIC RECOMMENDATIONS FOR RESOURCE OPTIMIZATION"""

# 7.1 PEAK DEMAND ANALYSIS
print("\n7.1 PEAK DEMAND ANALYSIS & RESOURCE ALLOCATION")

peak_analysis = df_clean.groupby(['HourOfCall', 'DayOfWeek']).agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean',
    'NumPumpsAttending': 'mean',
    'Notional Cost (£)': 'sum'
}).reset_index()

# Identify peak periods
peak_periods = peak_analysis.nlargest(10, 'IncidentNumber')
print("TOP 10 PEAK DEMAND PERIODS:")
day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
for i, row in peak_periods.iterrows():
    day_name = day_names[int(row['DayOfWeek'])]
    print(f"  {day_name} {int(row['HourOfCall'])}:00 - {int(row['IncidentNumber'])} incidents, "
          f"avg response: {row['FirstPumpArriving_AttendanceTime']:.1f}s")

# 7.2 GEOGRAPHIC OPTIMIZATION
print("\n7.2 GEOGRAPHIC OPTIMIZATION RECOMMENDATIONS")

geographic_efficiency = df_clean.groupby('IncGeo_BoroughName').agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean',
    'Distance_from_center': 'mean',
    'NumPumpsAttending': 'mean'
}).reset_index()

# Identify underperforming areas
slow_response_areas = geographic_efficiency.nlargest(5, 'FirstPumpArriving_AttendanceTime')
print("AREAS WITH SLOWEST RESPONSE TIMES (Need Additional Resources):")
for i, row in slow_response_areas.iterrows():
    print(f"  {row['IncGeo_BoroughName']}: {row['FirstPumpArriving_AttendanceTime']:.1f}s avg response, "
          f"{int(row['IncidentNumber'])} incidents")

# 7.3 RESOURCE DEPLOYMENT OPTIMIZATION
print("\n7.3 RESOURCE DEPLOYMENT OPTIMIZATION")

# Analyze pump utilization efficiency
pump_efficiency = df_clean.groupby('NumPumpsAttending').agg({
    'IncidentNumber': 'count',
    'FirstPumpArriving_AttendanceTime': 'mean',
    'Notional Cost (£)': 'mean',
    'PumpMinutesRounded': 'mean'
}).reset_index()

print("PUMP UTILIZATION ANALYSIS:")
for i, row in pump_efficiency.iterrows():
    efficiency_score = row['IncidentNumber'] / (row['FirstPumpArriving_AttendanceTime'] / 100)
    print(f"  {int(row['NumPumpsAttending'])} pumps: {int(row['IncidentNumber'])} incidents, "
          f"{row['FirstPumpArriving_AttendanceTime']:.1f}s response, "
          f"efficiency score: {efficiency_score:.2f}")

# 7.4 PREDICTIVE MAINTENANCE RECOMMENDATIONS
print("\n7.4 PREDICTIVE MAINTENANCE & COST OPTIMIZATION")

cost_analysis = df_clean.groupby(['PropertyCategory', 'IncidentGroup']).agg({
    'Notional Cost (£)': ['mean', 'sum', 'count'],
    'PumpMinutesRounded': 'mean'
}).round(2)

high_cost_scenarios = df_clean.groupby('PropertyType').agg({
    'Notional Cost (£)': 'mean',
    'IncidentNumber': 'count',
    'HasSecondPump': 'mean'
}).sort_values('Notional Cost (£)', ascending=False).head(5)

print("HIGH-COST INCIDENT SCENARIOS (Priority for Prevention):")
for prop_type, row in high_cost_scenarios.iterrows():
    print(f"  {prop_type}: £{row['Notional Cost (£)']:.0f} avg cost, "
          f"{int(row['IncidentNumber'])} incidents, "
          f"{row['HasSecondPump']:.1%} require second pump")

"""# 8. COMPREHENSIVE STRATEGIC RECOMMENDATIONS"""

recommendations = {
    "IMMEDIATE ACTIONS (0-3 months)": [
        "Deploy additional resources during peak hours: 10:00-12:00 and 15:00-18:00",
        f"Focus on {slow_response_areas.iloc[0]['IncGeo_BoroughName']} borough - highest response times ({slow_response_areas.iloc[0]['FirstPumpArriving_AttendanceTime']:.1f}s)",
        "Implement predictive dispatch system using the trained ML models",
        "Optimize pump allocation - current average utilization shows room for improvement"
    ],

    "SHORT-TERM IMPROVEMENTS (3-12 months)": [
        "Establish mobile response units for high-demand geographic clusters",
        "Implement dynamic resource reallocation based on hourly demand forecasts",
        "Develop specialized response protocols for high-cost property types",
        "Create early warning system for incident type prediction"
    ],

    "LONG-TERM STRATEGIC INITIATIVES (1-3 years)": [
        "Build new fire stations in underserved areas with high response times",
        "Implement IoT sensors for real-time incident detection and prevention",
        "Develop community engagement programs for high-risk property types",
        "Create integrated emergency response coordination center"
    ],

    "TECHNOLOGY INVESTMENTS": [
        "Real-time incident forecasting dashboard using the developed ML models",
        "Geographic optimization system for resource placement",
        "Predictive maintenance system for equipment and vehicles",
        "Advanced analytics platform for continuous improvement"
    ]
}

for category, items in recommendations.items():
    print(f"\n{category}:")
    for i, item in enumerate(items, 1):
        print(f"  {i}. {item}")

# 7.5 PERFORMANCE METRICS AND KPIs
print("\n" + "="*80)
print("9. KEY PERFORMANCE INDICATORS (KPIs) FOR MONITORING")
print("="*80)

current_kpis = {
    "Average Response Time": f"{df_clean['FirstPumpArriving_AttendanceTime'].mean():.1f} seconds",
    "Incidents Requiring Second Pump": f"{df_clean['HasSecondPump'].mean():.1%}",
    "Average Cost per Incident": f"£{df_clean['Notional Cost (£)'].mean():.0f}",
    "Peak Hour Utilization": f"{peak_analysis['IncidentNumber'].max()} incidents/hour",
    "Geographic Coverage Efficiency": f"{geographic_efficiency['FirstPumpArriving_AttendanceTime'].std():.1f}s std dev"
}

target_improvements = {
    "Average Response Time": "Reduce by 15% (target: 220s)",
    "Incidents Requiring Second Pump": "Reduce by 10% through better first response",
    "Average Cost per Incident": "Reduce by 12% through prevention",
    "Peak Hour Coverage": "Improve capacity by 25%",
    "Geographic Equity": "Reduce response time variation by 20%"
}

print("CURRENT PERFORMANCE:")
for kpi, value in current_kpis.items():
    print(f"  {kpi}: {value}")

print("\nTARGET IMPROVEMENTS:")
for kpi, target in target_improvements.items():
    print(f"  {kpi}: {target}")

"""# 10. SAVE RESULTS AND MODELS"""

"""# 10. SAVE RESULTS AND MODELS"""

print("\n" + "="*80)
print("10. SAVING RESULTS AND MODELS")
print("="*80)

# Save processed data
df_clean.to_csv('processed_fire_incidents_comprehensive.csv', index=False)

# Save model performance results
results_summary = {
    'Frequency_Forecasting': freq_results,
    'Response_Time_Prediction': rt_results,
    'Incident_Type_Classification': type_results
}

# Save feature importance
if 'feature_importance' in locals():
    feature_importance.to_csv('feature_importance_comprehensive.csv', index=False)

# Save recommendations
recommendations_df = pd.DataFrame([
    {'Category': cat, 'Recommendation': rec}
    for cat, recs in recommendations.items()
    for rec in recs
])
recommendations_df.to_csv('strategic_recommendations.csv', index=False)

# Save clustering results
if len(location_clusters) > 0:
    location_results = X_location.copy()
    location_results['Cluster'] = location_clusters
    location_results.to_csv('geographic_clusters.csv', index=False)

# ADD THIS NEW CODE BLOCK HERE:
# Save trained models
import joblib
import os

# Create models directory if it doesn't exist
os.makedirs('models', exist_ok=True)

print("Saving trained models...")

# Save frequency forecasting models
for model_name, model in frequency_models.items():
    model_filename = f"models/frequency_{model_name.lower().replace(' ', '_')}_model.pkl"
    joblib.dump(model, model_filename)
    print(f"✓ Saved {model_name} frequency model to {model_filename}")

# Save response time prediction models
for model_name, model in rt_models.items():
    model_filename = f"models/response_time_{model_name.lower().replace(' ', '_')}_model.pkl"
    joblib.dump(model, model_filename)
    print(f"✓ Saved {model_name} response time model to {model_filename}")

# Save incident type classification models
for model_name, model in type_models.items():
    model_filename = f"models/incident_type_{model_name.lower().replace(' ', '_')}_model.pkl"
    joblib.dump(model, model_filename)
    print(f"✓ Saved {model_name} incident type model to {model_filename}")

# Save clustering models
joblib.dump(kmeans_location, 'models/kmeans_location_model.pkl')
joblib.dump(dbscan_location, 'models/dbscan_location_model.pkl')
print("✓ Saved K-means and DBSCAN clustering models")

# Save scalers
joblib.dump(scaler_type, 'models/scaler_type.pkl')
joblib.dump(scaler_rt, 'models/scaler_response_time.pkl')
print("✓ Saved feature scalers")

# Save label encoders
joblib.dump(label_encoders, 'models/label_encoders.pkl')
print("✓ Saved label encoders")

print(f"\nAll models saved successfully in 'models/' directory")

print("\n=== COMPREHENSIVE ANALYSIS COMPLETE ===")
print("Files saved:")
print("- processed_fire_incidents_comprehensive.csv: Enhanced dataset")
print("- feature_importance_comprehensive.csv: ML feature rankings")
print("- strategic_recommendations.csv: Action items")
print("- geographic_clusters.csv: Location clustering results")
print("- models/: Directory containing all trained ML models")